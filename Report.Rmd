---
title: "Statistical Models Report"
Output: 
  pdf_document:
    latex_engine: xelatex
---

Part 1 - Time Travel Data
---

##  Part  (a)
> Create a subject-level data set (492 rows)
> with the following variables: the above
> seven predictors, the number of observations
> for the given subject (up to 18), and his/her
> number of future thoughts (i.e., number of observations
> for which future=1). (Hint: The
> aggregate() function or dplyr::group_by() may be useful.)

```{r}
source("renv/activate.R")
library(dplyr)
library(tidyverse)
library(haven)
library(zeallot)
```

### a1. Loading the data
```{r}
path <- "datasources-time-travel/Study\ 1/ETT_ESM_Study1.sav"
print(sprintf("Reading %s", path))
tib <- haven::read_sav(path)
# colnames(tib) # commented out - Too long output for report
```

### a2. Initial cleaning
```{r}
tib_clean <- tib |>
  select(where(~ !all(is.na(.))))

tib_clean |> group_by(Subject)
length(tib_clean)
```
### a3.  Subject Level

```{r}
subj_level <- tib_clean |>
  group_by(Subject) |>
  summarise(
    age = first(age, na_rm = TRUE),
    sex = first(sex, na_rm = TRUE),
    O = first(O, na_rm = TRUE),
    C = first(C, na_rm = TRUE),
    E = first(E, na_rm = TRUE),
    A = first(A, na_rm = TRUE),
    N = first(N, na_rm = TRUE),
    n_obs = n(),
    n_futures = sum(time_3, na.rm = TRUE)
  ) |>
  drop_na(age, sex, O, C, E, A, N)
```

> Use descriptive statistics
> and graphs to inspect these variables,
> and describe any missing data.

```{r}
table(subj_level$age, useNA = "ifany")
table(subj_level$sex, useNA = "ifany")
sapply(
  subj_level[, c("age", "O", "C", "E", "A", "N", "n_obs", "n_futures")],
  function(x) {
    c(
      Mean = mean(x, na.rm = TRUE),
      SD = sd(x, na.rm = TRUE),
      Min = min(x, na.rm = TRUE),
      Max = max(x, na.rm = TRUE)
    )
  }
)
par(mfrow = c(3, 3))
hist(subj_level$age, main = "Age", xlab = "Age", col = "darkblue")
hist(subj_level$O, main = "O", xlab = "O", col = "purple")
hist(subj_level$C, main = "C", xlab = "C", col = "brown")
hist(subj_level$E, main = "E", xlab = "E", col = "darkgreen")
hist(subj_level$A, main = "A", xlab = "A", col = "orange")
hist(subj_level$N, main = "N", xlab = "N", col = "pink")
```

## Part (b) 
> Using your favorite model selection method,
> find an appropriate Poisson regression
> with number of future thoughts as response
> and a subset of the above seven predictors.
> Include an offset term to account for the different
> numbers of observations per person.

### Chosen Statistical model

A Poisson regression model is fitted to the count of future events (`n_futures`).

- The expected count is modeled as a linear function of `sex` `O`, `C`, `E`, `A` and, `N`.
- The **log** is the link function for this GLM.
- The **offset ** `log(n_obs)` accounts for differing numbers of observations per subject.

#### Model Equation

Let \(y_i\) denote the number of future thoughts for subject \(i\), and \(n_i\) the number of observations recorded for that subject.  

The Poisson regression with offset is:

$$
y_i \sim \text{Poisson}(\mu_i), \quad
\log(\mu_i) = \log(n_i) + \beta_0 + \beta_1 \text{sex}_i + \beta_2 O_i + \beta_3 C_i + \beta_4 E_i + \beta_5 A_i + \beta_6 N_i
$$

$$
\begin{aligned}
y_i &= n\_futures_i && \text{(count of future thoughts)} \\
n_i &= n\_obs_i && \text{(number of observations)} \\
\beta_0, \dots, \beta_6 &= \text{coefficients for } \text{age}_i, \text{sex}_i, O_i, C_i, E_i, A_i, N_i
\end{aligned}
$$

```{r}
model <- glm(n_futures ~ age + sex + O + C + E + A + N,
  data = subj_level,
  family = poisson(link = "log"),
  offset = log(n_obs)
)
summary(model)
```

## Part (c) 
> Test for overdispersion in the chosen model,
>  and refit the same model with the “quasipoisson” family.
>  Does this change the model results? Why or why not?

### c1. Cheking for overdispersion
For each subject, this model makes the following assumption:
$$
\mathbb{E}(count) = \mathbb{V}(count)
$$
So we compute the dispersion:
$$
\hat{\phi}=\frac{\text{deviance}}{\text{tib}}
$$
We check for overdispersion if $\hat{\phi} > 1.5$

```{r}
disp <- deviance(model) / df.residual(model)
if (disp > 1.5) {
  print(sprintf("Overdispersion detected: (%s)", disp))
} else {
  print(sprintf("No Overdispersion detected: (%s)", disp))
}
```
### c2. Quasi-Poisson
The quasi-Poiison has a more relaxed variance assumption


```{r}
quasi_model <- glm(n_futures ~ sex + O + C + E + A + N,
  data = subj_level,
  family = quasipoisson(link = "log"),
  offset = log(n_obs),
)

quasi_disp <- summary(quasi_model)$dispersion
if (quasi_disp > 1.5) {
  print(sprintf("Quasi: Overdispersion detected: (%s)", quasi_disp))
} else {
  print(sprintf("Quasi: No Overdispersion detected: (%s)", quasi_disp))
}
```

Part 2 - Lucia Deberk
---


> **Generation of data.**
> Set the total number of shifts to `1029`,
>  where a third of
> them correspond to the morning shifts,
>  and all the rest to evening/night shifts.
> Denote the binary indicator of
> the morning shift by morning.
> Given morning = `1`
> generate a binary indicator Lucia
> of whether Lucia was on duty from `Bern(0.4)`,
> otherwise from `Bern(0.1)`.
> If you aggregate these two indicators into a 2-by-2 table,
>  you will get something
> similar to:

```{r}
shifts <- 1029

morning <- sample(c(FALSE, TRUE),
    size = shifts,
    replace = TRUE,
    prob = c(2 / 3, 1 / 3)
)

lucia <- ifelse(
    morning == TRUE,
    rbinom(n = shifts, size = 1, prob = 0.4),
    rbinom(n = shifts, size = 1, prob = 0.1)
) == 1

tb <- tibble(
    morning = morning,
    Lucia = lucia
)

tb |> count(Lucia, morning)

table(Lucia = tb$Lucia, Morning = tb$morning) |> addmargins()
```

> Now, generate the number of incidences (deaths)
>  occurred for each of the combi-
> nations of Lucia and morning,
>  from the following Poisson regression model
>  with a log link and an offset for the number of shifts

$$
 log \big( \frac{\mu (x_1, x_2)}{t(x_1, x_2)} \big)
 = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$

> Where $X_1$ = _morning_, $X_2$=_Lucia_ $t(x_1, x_2)$ equals the number of shifts corresponding to $(x_1, x_2)$ and,
> with $\beta_0 = -4, \beta_1 = 1.7, \beta_2 = 0$.

```{r}
c(beta_0, beta_1, beta_2) %<-% c(-4.0, 1.7, 0.0)

cell_counts <- tb |> count(morning, Lucia, name = "shift_count")
```


We want to find 
\(\mu (x_1, x_2) = \) Expected number of deaths
$$
log \big( \frac{\mu (x_1, x_2)}{t(x_1, x_2)} \big)
= \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$
$$
 \implies \frac{\mu (x_1, x_2)}{t(x_1, x_2)}
 = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2}
$$
$$
 \implies \mu (x_1, x_2)
 = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2}\cdot t(x_1, x_2)
$$

```{r}
tb2 <- cell_counts |>
    mutate(mu = shift_count * exp(beta_0 + beta_1 * morning + beta_2 * Lucia))

# Now apply the poission to the $\mu$
tb2 <- tb2 |> mutate(deaths = rpois(n = n(), lambda = mu))
tb2

lucia_marginal <- tb2 |>
    group_by(Lucia) |>
    summarise(
        deaths = sum(deaths),
        shifts = sum(shift_count),
    )
```

> (b) **Careful analysis**.
> Analyze the obtained count data
> using the Poisson log-linear regression with an
>  intercept and 2 factors
> $X_1 = morning$ and $X_2 = Lucia$, and
> include an offset term to account for the
> different number of shifts between Lucia
> de Berk and the rest of the nurses.
> Keep the pvalue corresponding to the effect of $X_2$.

```{r}
tb2 <- tb2 |>
    mutate(
        morning = as.integer(morning),
        Lucia   = as.integer(Lucia)
    )

y <- tb2$deaths

fit_b <- glm(
    deaths ~ morning + Lucia,
    family = poisson(link = "log"),
    data = tb2,
    offset = log(shift_count)
)


lucia_pvalue <- summary(fit_b)$coefficients["Lucia", "Pr(>|z|)"]
morning_pvalue <- summary(fit_b)$coefficients["morning", "Pr(>|z|)"]
print(sprintf("P values: Morning: %s Lucia: %s", morning_pvalue, lucia_pvalue))
```

> (c) **Less careful analysis.** Aggregate the data over
> $X_1 = morning$ in order to obtain similar data as
>  the real data set in Lucia de Berk’s trial.
> Analyze the obtained count data using again
> the Poisson log-linear regression with an intercept,
> a factor of $X_2 = Lucia$, and also include
> an offset term to account for the different
> number of shifts between Lucia de Berk
> and the rest of the nurses. Keep the pvalue
> corresponding to the effect of $X_2$.

```{r}
agg <- tb2 |>
    group_by(Lucia) |>
    summarise(
        deaths = sum(deaths),
        shift_count = sum(shift_count)
    )


fit_c <- glm(
    deaths ~ Lucia,
    family = poisson(link = "log"),
    data = agg,
    offset = log(shift_count)
)

#summary(fit_c)$coefficients["Lucia", "Pr(>|z|)"]

```
> (d) **Original analysis.**
> Under the null hypothesis that there is no difference between
> Lucia de Berk and other nurses, and under several additional assumptions, with
> one of them being that there are no other imoportant factors to be taken into
> account (aka confounders)
> (and also under blinded/fair data collection process),
> the conditional probability
> (given the total number of incidents and the total number
> of shifts) of observing the number of incidences
> (q) which was observed, or more,
> 3(7) (."2)/ ("Ee")
> could be calculated using the hypergeometric
> distribution (see page 235 in Meester
> et al. (2006)) summed up for all $x$ values which are $\geq x$ and $\leq k$:

$$
 \frac{\binom{m}{x}\binom{n}{k-x}}{\binom{m+n}{k}}
$$

> where $k$ is the number of incidents occurred,
> $n$ is the number of shifts of other
> nurses, $m$ is the number of shifts of Lucia,
> $x$ is the number of incidents ocurred
> during the shifts of Lucia. Keep the obtained pvalue.

> Summarize the results of 200 replications by calculating
>  the proportion of times you
> rejected the null hypothesis that there is no difference
>  between Lucia de Berk and other
> nurses (i.e., no nurse effect),
> for each type of the 3 analyses separately. Use the 0.05
> significance level for rejection of the null hypothesis.


**Breakdown:**

\(n =\)   number of shifts worked by Lucia
\(m =\)   number of shifts worked by Other Nurses
\(m+n =\) Total shifts 
\(k =\)   number of incidents
\(x =\)   number of incidents in Lucia's shift

\(H_0 =\) Lucia has the same rate as Other Nurses

$$
 P(X=x)=\frac{\binom{m}{x}\binom{n}{k-x}}{\binom{m+n}{k}}
$$

**Explanation:**

1. $\binom{m}{x}$: choose which $x$ of Lucia's $m$ shifts had incidents
2. $\binom{n}{k-x}$: choose which
$k-x$ of the other nurseses $n$ shifts had incidents
3. divide by $\binom{m+n}{k}$: all ways to place $k$
incidents among all shifts

```{r}
n <- tb2 |>
    filter(Lucia == FALSE) |>
    summarise(shift_count = sum(shift_count)) |>
    pull()

m <- tb2 |>
    filter(Lucia == TRUE) |>
    summarise(shift_count = sum(shift_count)) |>
    pull()

x <- tb2 |>
    filter(Lucia == TRUE) |>
    summarise(deaths = sum(deaths)) |>
    pull()


k <- tb2 |>
    summarise(deaths = sum(deaths)) |>
    pull()

cat(n, m, x, k, sep = "\n")

p_value <- phyper(x - 1, m, n, k, lower.tail = FALSE)

p_value
```

Simulation section 
---
#### Model Equations 

Let \(y_{ij}\) be the number of deaths where:

- \(i = 1\) := morning shift
- \(j = 1\) := Lucia de Berk on duty  
(and 0 is inverse)  
and \(t_{ij}\) denote the number of shifts.  

The model:

\[
y_{ij} \sim \text{Poisson}(\mu_{ij}), \quad
\log(\mu_{ij}) = \log(t_{ij}) + \beta_0 + \beta_1 \cdot i + \beta_2 \cdot j
\]



```{r}
NUM_SIMULATIONS <- 200
P_THRESH <- 0.05

gen_data <- function(shifts = 1029,
                     beta_0 = -4.0,
                     beta_1 = 1.7,
                     beta_2 = 0.0,
                     mornings_part = 1 / 3) {
    morning <- sample(c(FALSE, TRUE),
        size = shifts,
        replace = TRUE,
        prob = c(2 / 3, 1 / 3)
    )
    lucia <- ifelse(
        morning == TRUE,
        rbinom(n = shifts, size = 1, prob = 0.4),
        rbinom(n = shifts, size = 1, prob = 0.1)
    ) == 1

    data <- tibble(
        morning = as.integer(morning),
        Lucia = as.integer(lucia)
    )

    data <- data |> count(morning, Lucia, name = "shift_count")

    data <- data |> mutate(mu = shift_count * exp(beta_0 + beta_1 * morning + beta_2 * Lucia))

    # Now apply the poission to the $\mu$
    data <- data |> mutate(deaths = rpois(n = n(), lambda = mu))
    data
}

sim_b <- function(data) {
    fit_b <- glm(
        deaths ~ morning + Lucia,
        family = poisson(link = "log"),
        data = data,
        offset = log(shift_count)
    )

    summary(fit_b)$coefficients["Lucia", "Pr(>|z|)"]
}
sim_c <- function(data) {
    agg <- data |>
        group_by(Lucia) |>
        summarise(
            deaths = sum(deaths),
            shift_count = sum(shift_count)
        )


    fit_c <- glm(
        deaths ~ Lucia,
        family = poisson(link = "log"),
        data = agg,
        offset = log(shift_count)
    )

    summary(fit_c)$coefficients["Lucia", "Pr(>|z|)"]
}

sim_d <- function(data) {
    n <- data |>
        filter(Lucia == FALSE) |>
        summarise(shift_count = sum(shift_count)) |>
        pull()

    m <- data |>
        filter(Lucia == TRUE) |>
        summarise(shift_count = sum(shift_count)) |>
        pull()

    x <- data |>
        filter(Lucia == TRUE) |>
        summarise(deaths = sum(deaths)) |>
        pull()


    k <- data |>
        summarise(deaths = sum(deaths)) |>
        pull()


    phyper(x - 1, m, n, k, lower.tail = FALSE)
}

counter_b <- 0
counter_c <- 0
counter_d <- 0
# We the number of simlutations that cross our P value Threshhold.
for (i in 1:NUM_SIMULATIONS) {
    d <- gen_data()
    counter_b <- counter_b + as.integer(sim_b(data = d) < P_THRESH)
    counter_c <- counter_c + as.integer(sim_c(data = d) < P_THRESH)
    counter_d <- counter_d + as.integer(sim_d(data = d) < P_THRESH)
}

cat(
    sprintf("Number of pvalues below threshhold (%s)\n", P_THRESH),
    "=====\n",
    sprintf(
        "Simulation b: %s/%s (%s percent)\n",
        counter_b, NUM_SIMULATIONS, (counter_b / NUM_SIMULATIONS) * 100
    ),
    sprintf(
        "Simulation c: %s/%s (%s percent)\n",
        counter_c, NUM_SIMULATIONS, (counter_c / NUM_SIMULATIONS) * 100
    ),
    sprintf(
        "Simulation d: %s/%s (%s percent)\n",
        counter_d, NUM_SIMULATIONS, (counter_d / NUM_SIMULATIONS) * 100
    )
)
```

#### Interpretation

Across 200 simulations, the proportion of rejections at the 0.05 level gives an estimate of Type I error / FP rate under each of the simulation's assumptions:

- Simulation (b): Poisson regression with both factors morning and Lucia gives p-values indicating Lucia does not significantly affects the death rate.  
Simulation b accounts for the effect of morning shift $\approx 5$ percent, Showing correct Type I error control - FP occur only by chance, as expected under the null hypothesis.  

- Simulation (c): Aggregating over morning shifts shows the effect of Lucia alone.  
Simulation c collapses over morning shifts introducing confounding, causing substantially higher FP rates. This demonstrates how ignoring known confounder can cause an effect even when none exists. 


- Simulation (d): Hypergeometric approach calculates the exact p-value under the null hypothesis that Lucia's rate equals other nurses.  
Simulation d assumes incidents are randomly distributed accross all types of shifts resulting and demonstrates both ignoring known confounders and use of a wrong sampling model. This results in the highest FP rate of the three.



